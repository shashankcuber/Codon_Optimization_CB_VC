import json
import torch
from torch.nn.utils.rnn import pad_sequence
from utils.CONSTANTS import AMINO_ACID_DICT, CODON_DICT, CODON_BOX_DICT



class DataPreprocessing:

    def __init__(self, aa_list, cds_list):
        # Initialize the class with the list of amino acid and cds sequences
        self.aa_list = aa_list
        self.cds_list = cds_list
        

    # Encrypt the sequence
    def encrypt(self, sequence, n):
        return ' '.join([sequence[i:i+n] for i in range(0, len(sequence), n)])

    # pad post sequences with 0's with maxm length sequence
    def pad(self, sequences, max_len = None):
        #padding the pytorch way !!
        padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)
        
        return padded_sequences

    def tokenize(self, sequences, aa=False):

        if aa:
            token_to_index = AMINO_ACID_DICT
        else:
            token_to_index = CODON_DICT

        # Convert sequences to their index representation
        sequences_index = [[token_to_index[token] for token in sequence.split(' ')] for sequence in sequences]
        return sequences_index, token_to_index

    # Preprocess data
    def preprocess(self):
        
        aa_words = [self.encrypt(aa_seq, 1) for aa_seq in self.aa_list]
        cds_words = [self.encrypt(cds_seq, 3) for cds_seq in self.cds_list]

        aa_seq_tokenized, aa_token_dict = self.tokenize(aa_words, aa=True)
        cds_seq_tokenized, cds_token_dict = self.tokenize(cds_words, aa=False)
        
       
        # padding the sequences as they can be of variable length
        aa_tokenized_padded = self.pad([torch.tensor(seq) for seq in aa_seq_tokenized])
        cds_tokenized_padded= self.pad([torch.tensor(seq) for seq in cds_seq_tokenized])


        return aa_tokenized_padded, cds_tokenized_padded, aa_token_dict, cds_token_dict

if __name__ == '__main__':
    data_file_path = './aa_cds_dict.json'
    with open(data_file_path) as f:
        cds_aa_dict = json.load(f)
    obj = DataPreprocessing(list(cds_aa_dict.values())[:2], list(cds_aa_dict.keys())[:2])
    aa_seq_tokenized, cb_seq_tokenized, aa_token_dict, cds_token_dict = obj.preprocess()